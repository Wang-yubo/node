### 12 HTTP的核心知识2

##### 12.1 服务器的概念简介

> 实际的Web服务器会做些什么 >>>
>
> 1. 建立连接——接受一个客户端连接，或者如果不希望与这个客户端建立连接，就将其关闭。
> 2. 接收请求——从网络中读取一条HTTP 请求报文。
> 3. 处理请求——对请求报文进行解释，并采取行动。
> 4. 访问资源——访问报文中指定的资源。
> 5. 构建响应——创建带有正确首部的HTTP 响应报文。
> 6. 发送响应——将响应回送给客户端。
> 7. 记录事务处理过程——将与已完成事务有关的内容记录在一个日志文件中。
>
> ![image-20200225165133418](..\images\image-20200225165133418.png)
>
> 
>
> 服务器代理的概念 >>>
>
> Web 代理（proxy）服务器是网络的中间实体。代理位于客户端和服务器之间，扮演“中间人”的角色，在各端点之间来回传送HTTP 报文
>
> ![image-20200225165327797](..\images\image-20200225165327797.png)
>
> 反向代理 >>>
>
> 假扮Web 服务器被称为替代物（surrogate）或反向代理（reverse proxy）的代理接收发给Web 服务器的真实请求，但与Web 服务器不同的是，它们可以发起与其他服务器的通信，以便按需定位所请求的内容。
>
> 简单来说 >>> 正向代理 , 隐藏真实客户端反向代理 , 隐藏真实服务器
>
> 
>
> 服务器缓存的概念 >>>
>
> 有很多客户端访问一个流行的原始服务器页面时，服务器会多次传输同一份文档，每次传送给一个客户端。一些相同的字节会在网络中一遍遍地传输。这些冗余的数据传输会耗尽昂贵的网络带宽，降低传输速度，加重Web 服务器的负载。有了缓存，就可以保留第一条服务器响应的副本，后继请求就可以由缓存的副本来应对了，这样可以减少那些流入/ 流出原始服务器的、被浪费掉了的重复流量。
>
> 缓存(缓存不仅仅是本机自身的缓存, 还有缓存服务器一说)可以解决的常见问题:
>
> 1. 带宽瓶颈
> 2. 瞬间拥塞
> 3. 距离时延(超远距离访问, 比如:中国访问美国的网站, 需要计算光速)
>
> 可以用已有的副本为某些到达缓存的请求提供服务。这被称为缓存命中（cache hit），其他一些到达缓存的请求可能会由于没有副本可用，而被转发给原始服务器。这被称为缓存未命中（cache miss）
>
> ![image-20200225170923073](..\images\image-20200225170923073.png)

##### 12.2 web爬虫的基本原理

> Web 机器人是能够在无需人类干预的情况下自动进行一系列Web 事务处理的软件程序
> 很多机器人会从一个Web 站点逛到另一个Web 站点，获取内容，跟踪超链，并对它们找到的数据进行处理。根据这些机器人自动探查Web 站点的方式，人们为它们起了一些各具特色的名字，比如“爬虫”、“蜘蛛”、“蠕虫”以及“机器人”等
>
> Web 爬虫是一种机器人，它们会递归地对各种信息性Web 站点进行遍历，获取第一个Web 页面，然后获取那个页面指向的所有Web 页面，然后是那些页面指向的所有Web 页面，依此类推。递归地追踪这些Web 链接的机器人会沿着HTML 超链创建的网络“爬行”，所以将其称为爬虫（crawler）或蜘蛛（spider）。
>
> ![image-20200301142633312](..\images\image-20200301142633312.png)
>
> - 爬虫开始访问的URL 初始集合被称作根集（root set）
> - 上图所示的扒取逻辑中,只要有A,G,S就可以扒取所有的页面
>
> 
>
> 避免环路 >>>
>
> ![image-20200301142918356](..\images\image-20200301142918356.png)
>
> 机器人在Web 上爬行时，要特别小心不要陷入循环，或环路（cycle）之中.机器人必须知道它们到过何处，以避免环路的出现。环路会造成机器人陷阱，这些陷阱会暂停或减缓机器人的爬行进程。
>
> 环路造成的不利影响有:
>
> 1. 将时间消耗与无限获取相同的页面上了
> 2. 每次循环的访问也是访问, 要是爬虫与服务器的连接良好, 那么循环访问就犹如是DOS攻击了
> 3. 爬取大量重复页面被称为dups, 这时的爬虫程序会被重复的内容所充斥
>
> 
>
> 记录访问历史 >>>
>
> 为了保证不陷入环路我们必须得记录访问的历史, 但是这个记录很是麻烦, 问题在于两个方面:
>
> 1. 链接的数目极为庞大, 现如今的web页面数以百亿计, 每次要在这么庞大的链接库里面查询是否已经访问, 那得需要非常优秀的搜索算法, 否则匹配一次就直接gg了
> 2. 数据的体积庞大, 我们简单算一下, 假设一个url平均是40字节, 那么仅仅100亿个url, 那么体积就是400gb!!!, 这么大的数据为了保证快速读取一般会加载到内存中, 这对服务器的配置消耗极大
>
> 为解决这些问题, 我们一般会用到以下一些有用的技术 :
>
> 1. 树和散列表
> 2. 有损的存在位图
> 3. 检查点(讲已访问的URL保存在硬盘)
> 4. 分类派遣机器人

##### 12.3 客户端识别和cookie

> 为什么要进行客户端识别, 以淘宝为例:
>
> 1. 个性化的问候
> 2. 有的放矢的推荐
> 3. 管理信息的存档
> 4. 记录会话
>
> 可以实现客户端识别的方式有很多, 比如:
>
> 1. 承载用户身份信息的 HTTP 首部 (太危险)
> 2. 客户端 IP 地址跟踪，通过用户的 IP 地址对其进行识别(IP不够用,动态的)
> 3. 用户登录，用认证方式来识别用户(配置首部的Authorization)
> 4. 胖 URL，一种在 URL 中嵌入识别信息的技术(也极其危险)
> 5. cookie
>
> cookie >>>
>
> - 可以笼统地将cookie 分为两类： 会话cookie 和持久cookie。会话cookie 是一种临时cookie，它记录了用户访问站点时的设置和偏好。用户退出浏览器时，会话cookie 就被删除了。持久cookie 的生存时间更长一些；它们存储在硬盘上，浏览器退出，计算机重启时它们仍然存在。通常会用持久cookie 维护某个用户会周期性访问的站点的配置文件或登录名。
> - 会话cookie 和持久cookie 之间唯一的区别就是它们的过期时间。
>
> ![image-20200301150256026](..\images\image-20200301150256026.png)
>
> - 用户首次访问Web 站点时，Web 服务器对用户一无所知Web 服务器希望这个用户会再次回来，所以想给这个用户“拍上”一个独有的cookie，这样以后它就可以识别出这个用户了。cookie 中包含了一个由名字= 值（name=value）这样的信息构成的任意列表，并通过Set-Cookie 或Set-Cookie2 HTTP 响应（扩展）首部将其贴到用户身上去。
> - 浏览器会记住从服务器返回的Set-Cookie 或Set-Cookie2 首部中的cookie 内容，并将cookie 集存储在浏览器的cookie 数据库中
>
> 
>
> 服务器认证 >>>
>
> HTTP 提供了一个原生的质询/ 响应（challenge/response）框架，简化了对用户的认证过程
>
> ![image-20200301150357725](..\images\image-20200301150357725.png)
>
> - Web 应用程序收到一条HTTP 请求报文时，服务器没有按照请求执行动作，而是以一个“认证质询”进行响应，要求用户提供一些保密信息来说明他是谁，从而对其进行质询。
> - 用户再次发起请求时，要附上保密证书（用户名和密码）。如果证书不匹配，服务器可以再次质询客户端，或产生一条错误信息。如果证书匹配，就可以正常完成请求了。
>
> 服务器认证的步骤 >>>
>
> ![image-20200301150452399](..\images\image-20200301150452399.png)
>
> 上图，用户请求了私人家庭相片 /family/jeff.jpg。
>
> 1. 在图 b 中，服务器回送一条 401 Authorization Required，对私人家庭相片进行密码质询，同时回送的还有WWW-Authenticate 首部。这个首部请求对Family 域进行基本认证。
> 2. 在图 c 中，浏览器收到了 401 质询，弹出对话框，询问 Family 域的用户名和密码。用户输入用户名和密码时， 浏览器会用一个冒号将其连接起来， 编码成“ 经过扰码的”Base-64 表示形式然后将其放在Authorization 首部中回送。
> 3. 在图 d 中，服务器对用户名和密码进行解码，验证它们的正确性，然后用一条HTTP 200 OK 报文返回所请求的报文。
>
> ![image-20200301150700815](..\images\image-20200301150700815.png)
>
> WWW-Authenticate质询中包含了一个realm 指令。Web 服务器会将受保护的文档组织成一个安全域（security realm）。每个安全域都可以有不同的授权用户集。
>
> ![image-20200301150733894](..\images\image-20200301150733894.png)
>
> HTTP 基本认证将（由冒号分隔的）用户名和密码打包在一起，并用Base-64 编码方式对其进行编码
>
> ![image-20200301150750466](..\images\image-20200301150750466.png)
>
> 
>
> 服务器基本认证的安全缺陷 >>>
>
> 基本认证简单便捷，但并不安全。只能用它来防止非恶意用户无意间进行的访问，或将其与SSL 这样的加密技术配合使用。
>
> 1. 基本认证会通过网络发送用户名和密码，这些用户名和密码都是以一种很容易解码的形式表示的。如果有动机的第三方用户有可能会去拦截基本认证发送的用户名和密码，就要通过SSL 加密信道发送所有的HTTP 事务，或者使用更安全的认证协议，比如摘要认证。
> 2. 即使密码是以更难解码的方式加密的，第三方用户仍然可以捕获被修改过的用户名和密码，并将修改过的用户名和密码一次一次地重放给原始服务器，以获得对服务器的访问权。没有什么措施可用来防止这些重放攻击。
> 3.  即使将基本认证用于一些不太重要的应用程序，比如公司内部网络的访问控制或个性化内容的访问，一些不良习惯也会让它变得很危险。很多用户由于受不了大量密码保护的服务，会在这些服务间使用相同的用户名和密码。比如说从免费的因特网邮件网站捕获明文形式的用户名和密码，然后会发现用同样的用户名和密码还可以访问重要的在线银行网站！
> 4. 假冒服务器进行欺骗, 以得到用户发送的账号和密码,并存储起来以供后期使用

